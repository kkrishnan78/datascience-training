<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>20-Dataframes - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":false,"maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"memory_mb":6144,"category":"Community Edition","num_cores":0.88,"support_ebs_volumes":false}],"default_node_type_id":"dev-tier-node"},"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":false,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":false,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-6c2dd678fff350c03ba0e945bab52d0080cd857a39c99a22131b3e824bb8096f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-6fb640835bd45a2e2095758663e237aefe80671acacc2e6377eec5ecccb9004b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-ca4b827492f83a5a1402629a8aa9e58d1ea1f0f6b1f81b1f1c8fffc7bab62ce9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-22ee201c31a037446476259c6efb835d5da16d8e49a677a6c532065503c14e0a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"p2.8xlarge":16,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":true,"enableNotebookHistoryDiffing":true,"branch":"2.37","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-dd209a93dfbacf46690f2f46369e1df041a4c7f7c51e9e7752f6106dfcca9295","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":true,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"TRAINING_BASIC_TIER","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"1c130e3f2eeb1563bb246dc9e8e275f3a9bd8e84","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":false,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3420809998616622,"name":"20-Dataframes","language":"scala","commands":[{"version":"CommandV1","origId":3420809998616624,"guid":"1efef190-5e96-4aea-a70b-bc472e4048b2","subtype":"command","commandType":"auto","position":1.0,"command":"%md\n# SparkSQL and DataFrames\n\n* Some form of SQL processing has been part of the core distribution since 1.0 (April 2014)\n  * The purpose is to runs SQL / HiveQL queries, optionally alongside or replacing existing Hive deployments\n\n* __Modern SparkSQL and DataFrames represent the same thing: a new language-independent, high-performance query engine implementation__\n  * SparkSQL/DataFrames is different from, and replaces, a variety of older approaches including Shark, Hive-on-Spark, and SchemaRDD\n  * Although Spark contains its own data processing engine, it can integrate closely with a Hive metastore\n\n<img src=\"http://i.imgur.com/kOZqeNo.png\" width=\"600\">","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0e98676a-526d-4822-bb34-16a4f7fc1536"},{"version":"CommandV1","origId":3420809998616625,"guid":"03c799c1-9e41-4f46-aafa-f817cf6b0764","subtype":"command","commandType":"auto","position":2.0,"command":"%md\n## SparkSQL, DataFrames and DataSets Represent the Same Component in Modern Spark\n\n##### This component allows major optimizations by Spark, far beyond what is possible with the RDD API, in terms of both execution speed and data storage size.\n\n* DataFrame/DataSet API and SQL (typically via the HiveQL parser) are equivalent ways to do the same tasks.\n  * SQL serves many general purposes, including analytic work via BI tools (over JDBC and the Thriftserver)\n  * DataFrames offer more programmatic control and an API familiar to users of Pandas or R\n  * DataSets are a generalization of DataFrames and the underlying engine, to support more data types and stronger type enforcement","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"66fea8d5-a944-4dbe-9c79-a233f1972bab"},{"version":"CommandV1","origId":3420809998616626,"guid":"9cf09def-eb90-4d37-95e1-4551d9076167","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n## DataFrame API\n\n* Enable wider audiences beyond Big Data engineers to leverage the power of distributed processing\n  * Seamless integration with all big data tooling and infrastructure via Spark\n  * Designed from the ground-up to support modern big data and data science applications\n\n* Inspired by data frames in R and Python (Pandas), but offering transparent scale-out support\n  * ... from kilobytes of data on a single laptop to petabytes on a large cluster\n\n* Support for a wide array of data formats and storage systems\n* State-of-the-art optimization and code generation through the Spark SQL Catalyst optimizer\n* APIs for Python, Java, Scala, and R\n\nSee\n* https://spark.apache.org/docs/latest/sql-programming-guide.html \n* http://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"45b7afed-d656-475e-819b-67f02b01680f"},{"version":"CommandV1","origId":3420809998616627,"guid":"65aae601-7491-4aae-869e-ade9d7d06998","subtype":"command","commandType":"auto","position":4.0,"command":"%md\n## DataFrames\nThe preferred abstraction in Spark (introduced in 1.3)\n* Strongly typed collection of distributed elements\n  * Built on Resilient Distributed Datasets\n* _Immutable once constructed_\n* Track lineage information to efficiently recompute lost data\n* Enable operations on collection of elements in parallel\n\nYou construct DataFrames\n* by parallelizing existing collections (e.g., Pandas DataFrames) \n* by transforming an existing DataFrame\n* from files in HDFS, Hive tables, or any other storage system (e.g., Parquet in S3)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bb21d8ad-f5cc-42a7-a5ea-f92b6ecba376"},{"version":"CommandV1","origId":3420809998616628,"guid":"9d082507-11af-450a-9e2d-b119e80f771c","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n## Why Use DataFrames instead of RDDs?\n\n\n* For new users familiar with data frames in other programming languages, this API should make them feel at home\n* For existing Spark users, the API will make Spark easier to program than using RDDs\n* For both sets of users, DataFrames will improve performance through intelligent optimizations and code-generation\n<br/>\n<br/>\n<br/>\n<img src=\"http://i.imgur.com/0uLWgHl.png\" width=\"600\">","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e51abff7-dde7-494b-8220-15202324dcde"},{"version":"CommandV1","origId":3420809998616629,"guid":"cea1e8d4-8253-42ff-b78d-6408c570f352","subtype":"command","commandType":"auto","position":6.0,"command":"%md\n## DataFrames can be significantly faster than RDDs. \n\nAnd they perform the same, regardless of language.\n\n<img src=\"http://i.imgur.com/CQCBB2E.png\" width=\"600\">","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1355974f-fd78-4f38-b1e5-6cc536509aa8"},{"version":"CommandV1","origId":3420809998616630,"guid":"597b8fa2-5ba0-4b93-8f71-b8aff7d85be6","subtype":"command","commandType":"auto","position":7.0,"command":"%md\n## Write Less Code: Input & Output\n\nUnified interface to reading/writing data in a variety of formats.\n\n```\nval df = sqlContext.\n  read.\n  format(\"json\").\n  option(\"samplingRatio\", \"0.1\").\n  load(\"/Users/spark/data/stuff.json\")\n\ndf.write.\n   format(\"parquet\").\n   mode(\"append\").\n   partitionBy(\"year\").\n   saveAsTable(\"faster-stuff\")\n```","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"53e09b95-f1c6-4e3b-bde8-3c2233a68a60"},{"version":"CommandV1","origId":3420809998616631,"guid":"c083010d-ab8d-4b15-8593-64d76a649553","subtype":"command","commandType":"auto","position":8.0,"command":"%md\n## Data Sources supported by DataFrames\n\n<img src=\"http://i.imgur.com/sdYuTIv.png\" width=\"600\">","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"09645430-45dd-4083-8e10-fd32b4d4bc81"},{"version":"CommandV1","origId":3420809998616632,"guid":"2ed90987-d9fd-4926-aac8-85b1aba723de","subtype":"command","commandType":"auto","position":9.0,"command":"%md ##Write Less Code: High-Level Operations\n\nSolve common problems concisely with DataFrame functions:\n* selecting columns and filtering\n* joining different data sources\n* aggregation (count, sum, average, etc.)\n* plotting results (e.g., with Pandas)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4ed68652-f54b-4eae-82cd-de78207ba738"},{"version":"CommandV1","origId":3420809998616633,"guid":"cf801400-2382-491b-abae-a3fd3ff49705","subtype":"command","commandType":"auto","position":10.0,"command":"%md ##Write Less Code: Compute an Average\n\n<img src=\"http://i.imgur.com/gLDgzDP.png\" width=\"600\">\n\n__The Spark code is much shorter ... but it's not clear. It's complex, and the programmer's intent is not communicated clearly.__\n\nLet's look at making that both *shorter* __and__ *easier*. First, we'll set up some data to use:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e1ed68e5-a820-46eb-a24e-909ed8b41cb0"},{"version":"CommandV1","origId":3420809998616634,"guid":"5b3f285b-4644-4cd7-ab9c-251712dd3ccd","subtype":"command","commandType":"auto","position":11.0,"command":"// set up RDD\nval data = sc.parallelize(List((\"Jim\", 30), (\"Anne\", 31), (\"Jim\", 32)))\n\n// set up DataFrame\nimport org.apache.spark.sql.functions._\ndata.toDF(\"name\", \"age\").createOrReplaceTempView(\"people\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">data: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[220] at parallelize at &lt;console&gt;:40\nimport org.apache.spark.sql.functions._\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477310636E12,"submitTime":1.486477310618E12,"finishTime":1.486477311215E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"85657351-e78f-4ba9-82d0-f0a4b5d76367"},{"version":"CommandV1","origId":3420809998616635,"guid":"fda15bf2-70fc-47b5-890c-7238c460af42","subtype":"command","commandType":"auto","position":12.0,"command":"%md Run the RDD-based Spark code to calculate average ages for each name:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ef568e98-26b7-4ac2-b9df-ee2f29d1ed84"},{"version":"CommandV1","origId":3420809998616636,"guid":"0bdb1f4f-ee5a-464f-9242-8e0fe848dc5d","subtype":"command","commandType":"auto","position":13.0,"command":"data.map { x => (x._1, (x._2, 1)) }\n  .reduceByKey { case (x,y) => \n      (x._1 + y._1, x._2 + y._2) }\n  .map { x => (x._1, x._2._1 / x._2._2) }\n  .collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res2: Array[(String, Int)] = Array((Anne,31), (Jim,31))\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477321233E12,"submitTime":1.486477321215E12,"finishTime":1.486477322685E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9c3ca679-2d34-4243-ac7b-bb08ab8e8650"},{"version":"CommandV1","origId":3420809998616637,"guid":"f765e36a-b050-4f75-8b61-ded530fa5a96","subtype":"command","commandType":"auto","position":14.0,"command":"%md Now the same calculation, using the DataFrame API.\n\nYou'll probably agree that the following is much easier to understand and harder to mess up when coding:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5f451bf0-be85-4f4f-aaca-5bbe035d887d"},{"version":"CommandV1","origId":3420809998616638,"guid":"df10fe94-433c-4b87-9126-a4ded8e4cc33","subtype":"command","commandType":"auto","position":15.0,"command":"sqlContext.table(\"people\")\n          .groupBy(\"name\")\n          .agg(avg(\"age\"))\n          .collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res3: Array[org.apache.spark.sql.Row] = Array([Jim,31.0], [Anne,31.0])\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477329078E12,"submitTime":1.486477329059E12,"finishTime":1.486477331406E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7731490f-c83a-4bc2-bc85-2e2d292e4f86"},{"version":"CommandV1","origId":3420809998616639,"guid":"5b642959-62af-46dd-9a7b-1513a654b812","subtype":"command","commandType":"auto","position":16.0,"command":"%md ###... and Spark can optimize it!\n\nThe full API Docs are here:\n\n* Scala - http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package\n* Java - http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/package-summary.html\n* Python - http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql\n* R - http://spark.apache.org/docs/latest/api/R/index.html\n\n__BUT for a shortcut to finding APIs you need, learning, or trying to solve a problem, bookmark (or at least look first at) the following items:__\n\nIn org.apache.spark.sql:\n1. DataFrame class\n2. Column class\n3. functions object\n4. GroupedData class","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4be07444-c45b-4e60-8b7b-402bbd8442fc"},{"version":"CommandV1","origId":3420809998616640,"guid":"f9fe14fa-fb72-468d-b46f-109a3fe3c0fe","subtype":"command","commandType":"auto","position":17.0,"command":"%md ##Create a DataFrame\n\nDataFrames need a schema: consistent columns, each with a name and a type.\n\nWe can create one from a Parquet file easily, because all of that schema info is built into the Parquet file:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c5a2225c-3e9d-4576-805f-e23ba63d6fd8"},{"version":"CommandV1","origId":3420809998616641,"guid":"238e908e-596b-4854-86ba-d43173e193c7","subtype":"command","commandType":"auto","position":18.0,"command":"val people = sqlContext.read.parquet(\"dbfs:/mnt/training/ssn/names.parquet\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">people: org.apache.spark.sql.DataFrame = [firstName: string, gender: string ... 2 more fields]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477347005E12,"submitTime":1.486477346987E12,"finishTime":1.486477348721E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"21fe91e3-0946-43dd-adf8-dc00b06528b4"},{"version":"CommandV1","origId":3420809998616642,"guid":"7e22484a-0d88-40c5-be8d-2ee724b3e901","subtype":"command","commandType":"auto","position":19.0,"command":"%md ## Use DataFrames","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b90a5b73-f721-452f-99df-8b2f13ae1afe"},{"version":"CommandV1","origId":3420809998616643,"guid":"e85e32a1-ca67-4dfe-8b95-f2283edf9932","subtype":"command","commandType":"auto","position":20.0,"command":"people.show(5)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---------+------+-----+----+\n|firstName|gender|total|year|\n+---------+------+-----+----+\n| Jennifer|     F|54336|1983|\n|  Jessica|     F|45278|1983|\n|   Amanda|     F|33752|1983|\n|   Ashley|     F|33292|1983|\n|    Sarah|     F|27228|1983|\n+---------+------+-----+----+\nonly showing top 5 rows\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.48647735756E12,"submitTime":1.486477357542E12,"finishTime":1.486477358815E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8f3f4c03-39af-40bf-a856-014446844fce"},{"version":"CommandV1","origId":3420809998616644,"guid":"c258f8e4-3195-4980-9d2c-c386c3ba1125","subtype":"command","commandType":"auto","position":21.0,"command":"var popular = people.filter($\"total\" > 80000)\npopular.show(500)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---------+------+-----+----+\n|firstName|gender|total|year|\n+---------+------+-----+----+\n|    James|     M|80248|1943|\n|    James|     M|87425|1946|\n|   Robert|     M|84130|1946|\n|    Linda|     F|99680|1947|\n|    James|     M|94755|1947|\n|   Robert|     M|91642|1947|\n|     John|     M|88318|1947|\n|    Linda|     F|96205|1948|\n|    James|     M|88596|1948|\n|   Robert|     M|85479|1948|\n|     John|     M|82826|1948|\n|    Linda|     F|91010|1949|\n|    James|     M|86856|1949|\n|   Robert|     M|83841|1949|\n|     John|     M|81154|1949|\n|    Linda|     F|80431|1950|\n|    James|     M|86221|1950|\n|   Robert|     M|83540|1950|\n|    James|     M|87175|1951|\n|   Robert|     M|86327|1951|\n|     John|     M|81512|1951|\n|    James|     M|87083|1952|\n|   Robert|     M|86572|1952|\n|     John|     M|83121|1952|\n|   Robert|     M|86135|1953|\n|    James|     M|85946|1953|\n|  Michael|     M|84164|1953|\n|     John|     M|80072|1953|\n|  Michael|     M|88485|1954|\n|    James|     M|86277|1954|\n|   Robert|     M|86258|1954|\n|     John|     M|81128|1954|\n|  Michael|     M|88283|1955|\n|    David|     M|86191|1955|\n|    James|     M|84130|1955|\n|   Robert|     M|83676|1955|\n|     John|     M|80018|1955|\n|  Michael|     M|90633|1956|\n|    James|     M|84860|1956|\n|   Robert|     M|83903|1956|\n|    David|     M|81603|1956|\n|     John|     M|80760|1956|\n|  Michael|     M|92709|1957|\n|    James|     M|84242|1957|\n|    David|     M|82367|1957|\n|   Robert|     M|81707|1957|\n|  Michael|     M|90519|1958|\n|    David|     M|82585|1958|\n|  Michael|     M|85272|1959|\n|    David|     M|83921|1959|\n|    David|     M|85931|1960|\n|  Michael|     M|84206|1960|\n|  Michael|     M|86916|1961|\n|    David|     M|84771|1961|\n|  Michael|     M|85046|1962|\n|    David|     M|81328|1962|\n|  Michael|     M|83795|1963|\n|  Michael|     M|82674|1964|\n|     John|     M|82536|1964|\n|  Michael|     M|81048|1965|\n|  Michael|     M|80002|1966|\n|  Michael|     M|82454|1967|\n|  Michael|     M|82019|1968|\n|  Michael|     M|85227|1969|\n|  Michael|     M|85321|1970|\n+---------+------+-----+----+\n\npopular: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [firstName: string, gender: string ... 2 more fields]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477397087E12,"submitTime":1.486477397068E12,"finishTime":1.486477398268E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4f4d4b58-6e8b-4705-a1bb-4ef6ed0db8a3"},{"version":"CommandV1","origId":3420809998616645,"guid":"41eee266-aad5-4945-b84d-2781cf9628e1","subtype":"command","commandType":"auto","position":22.0,"command":"val df = people.select($\"firstName\", $\"total\" * 1000)\ndf.show\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+---------+--------------+\n|firstName|(total * 1000)|\n+---------+--------------+\n| Jennifer|      54336000|\n|  Jessica|      45278000|\n|   Amanda|      33752000|\n|   Ashley|      33292000|\n|    Sarah|      27228000|\n|  Melissa|      23472000|\n|   Nicole|      22392000|\n|Stephanie|      22323000|\n|  Heather|      20749000|\n|Elizabeth|      19838000|\n|  Crystal|      17904000|\n|      Amy|      17095000|\n| Michelle|      16828000|\n|  Tiffany|      15960000|\n| Kimberly|      15374000|\n|Christina|      15359000|\n|    Amber|      14886000|\n|     Erin|      14835000|\n|  Rebecca|      14711000|\n|   Rachel|      14588000|\n+---------+--------------+\nonly showing top 20 rows\n\ndf: org.apache.spark.sql.DataFrame = [firstName: string, (total * 1000): int]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477456389E12,"submitTime":1.486477456371E12,"finishTime":1.486477457178E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b8ed18b9-b162-4570-991b-e6a39d19f0f1"},{"version":"CommandV1","origId":3420809998616646,"guid":"0b5ed343-7769-4b67-b439-96c2810d52be","subtype":"command","commandType":"auto","position":23.0,"command":"%md Many DataFrame API functions have overloads that can take Column objects or SQL string column names (select) or even expressions (filter):","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3d0afa9b-661c-4e31-92cd-eae503eac114"},{"version":"CommandV1","origId":3420809998616647,"guid":"806e5dd7-b502-4798-ac57-a96790407810","subtype":"command","commandType":"auto","position":24.0,"command":"println(people.filter(\"total < 80000\"))\nprintln(people.select(\"firstName\", \"total\"))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">[firstName: string, gender: string ... 2 more fields]\n[firstName: string, total: int]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477480021E12,"submitTime":1.486477480004E12,"finishTime":1.486477480345E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5c23bbd3-fec0-4c49-8fe6-2d36f35f6cfb"},{"version":"CommandV1","origId":3420809998616648,"guid":"876c56cb-8da9-43a8-9d05-2c10d34f88fa","subtype":"command","commandType":"auto","position":25.0,"command":"%md We'll learn more about columns and expressions in a bit. For now, here are a couple more examples of logic (grouping/counting, joining) using the DataFrame API:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a291d1dc-e2b1-45f5-9eac-670ef842afe9"},{"version":"CommandV1","origId":3420809998616649,"guid":"4e9fd455-dd7e-4b78-9bc4-92bf57ec5331","subtype":"command","commandType":"auto","position":26.0,"command":"people.groupBy(\"year\").count","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">res10: org.apache.spark.sql.DataFrame = [year: int, count: bigint]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477511031E12,"submitTime":1.486477511013E12,"finishTime":1.486477511259E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7b41149f-c689-4a53-ae3b-b90e3b9d8a3a"},{"version":"CommandV1","origId":3420809998616650,"guid":"1e461600-39a4-4b89-a994-8e5dbde92153","subtype":"command","commandType":"auto","position":27.0,"command":"val scores = sc.parallelize(List( (\"James\", 100), (\"Linda\", 100) )).toDF(\"name\", \"score\")\n\npeople.join(scores, people(\"firstName\") === scores(\"name\"), \"inner\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">scores: org.apache.spark.sql.DataFrame = [name: string, score: int]\nres11: org.apache.spark.sql.DataFrame = [firstName: string, gender: string ... 4 more fields]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477515955E12,"submitTime":1.486477515937E12,"finishTime":1.48647751632E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"17b9af85-5cdf-45a8-ab44-e152813bca4f"},{"version":"CommandV1","origId":3420809998616651,"guid":"6df72510-7b5f-48a7-9278-774f08878618","subtype":"command","commandType":"auto","position":28.0,"command":"%md ## Connecting DataFrames and SparkSQL","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"eeeedc55-e16b-4858-b42d-f9d7ac632c64"},{"version":"CommandV1","origId":3420809998616652,"guid":"d3ece857-fe0a-4e11-8426-a75bcfb9825b","subtype":"command","commandType":"auto","position":29.0,"command":"people.createOrReplaceTempView(\"people\")\nsqlContext.sql(\"SELECT count(*) FROM people\").show()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">+--------+\n|count(1)|\n+--------+\n| 1825433|\n+--------+\n\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.486477566996E12,"submitTime":1.486477566978E12,"finishTime":1.486477567596E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ae5bd4e7-60a2-4e27-ba68-b1f1f5d6e6f9"},{"version":"CommandV1","origId":3420809998616653,"guid":"0d69f118-b222-4e9e-9d6f-084c17f715ec","subtype":"command","commandType":"auto","position":30.0,"command":"%md Note that registerTempTable just creates a symbol representing the query (a bit like a SQL View) so that, later, we can use SQL to reference that query. We can treat it like a table, and the parser will understand what we mean. These symbols (\"tables\") are available to any code using the sqlContext -- they can also access this table directly if they like (without SQL) by calling ```sqlContext.table(\"people\")```","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a8d65243-3318-4a0f-bda1-9ca4504c9126"},{"version":"CommandV1","origId":3420809998616654,"guid":"80208fa2-bc84-436e-8167-4837981ae571","subtype":"command","commandType":"auto","position":31.0,"command":"%md ## DataFrames and SparkSQL\n\nThe DataFrames API provides a programmatic interface; really, a domain-specific language (DSL) for interacting with your data.\n* Spark SQL allows you to manipulate distributed data with SQL queries. Currently, two SQL dialects are supported.\n  * If you're using a Spark SQLContext, the only supported dialect is `sql`, a rich subset of SQL 92.\n  * If you're using a HiveContext, the default dialect is \"hiveql\", corresponding to Hive's SQL dialect. `sql` is also available, but `hiveql` is a richer dialect.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"086a4e0b-b5ac-41f9-84ef-bad52f1dcae7"},{"version":"CommandV1","origId":3420809998616655,"guid":"fbbdd2f6-0ced-43e4-a0cd-3e724d7dd996","subtype":"command","commandType":"auto","position":32.0,"command":"%md ##SparkSQL\n\nYou issue SQL queries through a SQLContext or HiveContext, using the sql() method.\n* The sql() method returns a DataFrame.\n* You can mix DataFrame methods and SQL queries in the same code.\n* To use SQL, you must either:\n  * query a persisted Hive table, or\n  * make a table alias for a DataFrame, using registerTempTable()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ff195fae-4d83-438b-b236-7c85fcacc500"},{"version":"CommandV1","origId":3420809998616656,"guid":"7ee204e8-2227-45f3-bb5c-6b10c312c1d5","subtype":"command","commandType":"auto","position":33.0,"command":"%md ##Transformations, Actions, Laziness\n\nDataFrames are lazy. Transformations contribute to the query plan, but they don't execute anything. \nActions cause the execution of the query.\n\n|Transformations|Actions|\n|---|---|\n|filter,select,drop,intersect,join|count,collect,show,head,take\n\n\n*Actions cause the execution of the query.*\n\n__What, exactly, does execution of the query mean? It means:__\n* Spark initiates a distributed read of the data source\n* The data flows through the transformations (the RDDs resulting from the Catalyst query plan)\n* The result of the action is pulled back into the driver JVM.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"75e9b8a2-d9e5-46af-bf11-28f1288b66dc"},{"version":"CommandV1","origId":3420809998616657,"guid":"55d18ec0-107b-482d-b134-1a6af5d7aefe","subtype":"command","commandType":"auto","position":34.0,"command":"people.take(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a169750a-45c1-4dc2-8ffb-1d8a6806c39a"},{"version":"CommandV1","origId":3420809998616658,"guid":"13c34cef-1ec2-48b0-a1f4-649ea08bf26c","subtype":"command","commandType":"auto","position":35.0,"command":"%md ##DataFrames have Schemas\n\nIn the previous example, we created DataFrames from Parquet and JSON data.\n* A Parquet table has a schema (column names and types) that Spark can use. Parquet also allows Spark to be efficient about how it pares down data.\n* Spark can infer a Schema from a JSON file.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e842ea32-d034-4767-8fa9-e5c8bb4ca108"},{"version":"CommandV1","origId":3420809998616659,"guid":"3918a27b-6455-45f8-afca-be6d51849345","subtype":"command","commandType":"auto","position":36.0,"command":"%md ##Columns\n\nWhen we say column here, what do we mean?\n\n* a DataFrame column is an abstraction. It provides a common column-oriented view of the underlying data, regardless of how the data is really organized.\n\n* Columns are important because much of the DataFrame API consists of functions that take or return columns (even if they don't look that way at first).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9daaa186-51d9-410e-bd38-36f34b743eed"},{"version":"CommandV1","origId":3420809998616660,"guid":"1bf41cda-f1a3-459b-8733-05d50b158657","subtype":"command","commandType":"auto","position":37.0,"command":"%md ## How Do Columns Map to Common Data Types?\n\n<img src=\"http://i.imgur.com/QfUf7Ub.png\" width=\"500\">","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1c454284-d118-4677-8933-26167f896055"},{"version":"CommandV1","origId":3420809998616661,"guid":"fc502a1f-56d7-4e09-bfcc-65b785eb350d","subtype":"command","commandType":"auto","position":38.0,"command":"%md #### SQL, CSV, JSON ...\n\n<img src=\"http://i.imgur.com/ajxRmWN.png\" width=\"720\">","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fa59f5ca-741e-4646-a1ab-3d8ebee5ac85"},{"version":"CommandV1","origId":3420809998616662,"guid":"8f92ba2c-97ed-4c3f-bdca-9732f01537a4","subtype":"command","commandType":"auto","position":39.0,"command":"%md ##Accessing Colums\n\nAssume we have a DataFrame, df, that reads a data source that has \"first\", \"last\", and \"age\" columns.\n\n|Python|Scala|Java|R|\n|---|---|---|---|\n|df[\"first\"], df.first|df(\"first\"), $\"first\"|df.col(\"first\")|df$first|\n\n\nIn Python, it's possible to access a DataFrame's columns either by attribute (df.age) or by indexing (df['age']). While the former is convenient for interactive data exploration, you should use the index form. It's future proof and won't break with column names that are also attributes on the DataFrame class.\n\nThe $ syntax can be ambiguous if there are multiple DataFrames in the lineage.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e04a64cf-403f-4950-ae63-2434563b5533"},{"version":"CommandV1","origId":3420809998616663,"guid":"f2a80235-9d68-45a2-ae74-a48d444bd46b","subtype":"command","commandType":"auto","position":40.0,"command":"%md ## printSchema()\n\nYou can have Spark tell you what it thinks the data schema is, by calling the printSchema() method.  (This is mostly useful in the shell.)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c391a017-b5f5-4dbd-9883-71f6ef5895cd"},{"version":"CommandV1","origId":3420809998616664,"guid":"0cb3f29a-2b84-434d-950d-545bf6bc2b1d","subtype":"command","commandType":"auto","position":41.0,"command":"people.printSchema","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c2a42e22-59eb-4136-9d2b-ab6583187102"},{"version":"CommandV1","origId":3420809998616665,"guid":"b0feb8b6-363b-4287-a71f-f40d5357ebbc","subtype":"command","commandType":"auto","position":42.0,"command":"%md ##Schema Inference\n\n* Some data sources (e.g., parquet) can expose a formal schema; others (e.g., plain text files) don't. How do we fix that?\n* You can create an RDD of a particular type and let Spark infer the schema from that type. We'll see how to do that in a moment.\n* You can use the API to specify the schema programmatically.\n\nThe key thing to remember is that a schema has consistent columns, column types, and column names. We can \"mix-and-match\" where these come from, as long as all three are there. For example, a CSV file might provide consistent columns and types, and we might fill in the column names; alternatively, a file might have column data and fields names, but we need to supply the types.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e1af1d3a-fe8a-4387-ab87-10677006c369"},{"version":"CommandV1","origId":3420809998616666,"guid":"1f7b5165-2680-4090-8585-a29bc93dc062","subtype":"command","commandType":"auto","position":43.0,"command":"%md ##Schema Application Example\n\nSuppose you have a file that looks like this:\n\n```\nErin,Shannon,F,42\nNorman,Lockwood,M,81\nMiguel,Ruiz,M,64\nRosalita,Ramirez,F,14\nAlly,Garcia,F,39\nClaire,McBride,F,23\nAbigail,Cottrell,F,75\nJosé,Rivera,M,59\nRavi,Dasgupta,M,25\n...\n```\n\nThe file has no schema, but it's obvious there is one:\n\n|Field|Type|\n|---|---|\n|First name|string|\n|Last name|string|\n|Gender|string|\n|Age|integer|\n\nLet's see how to get Spark to infer that schema.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"47843367-2450-445d-b787-1ccff359205b"},{"version":"CommandV1","origId":3420809998616667,"guid":"15fe497d-14a2-4663-b059-a538e469995a","subtype":"command","commandType":"auto","position":44.0,"command":"case class Person(firstName: String,\n                  lastName:  String,\n                  gender:    String,\n                  age:       Int)\n\nval rdd = sc.textFile(\"people.csv\")\n\nval peopleRDD = rdd.map { line =>\n  val cols = line.split(\",\")\n  Person(cols(0), cols(1), cols(2), cols(3).toInt)\n}\n\nval df = peopleRDD.toDF\n\n// df: DataFrame = [firstName: string, lastName: string, gender: string, age: int]","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"79945705-18f9-42fe-8532-cee03986acac"},{"version":"CommandV1","origId":3420809998616668,"guid":"81feff4f-20c2-4644-a112-10e0781a45f4","subtype":"command","commandType":"auto","position":45.0,"command":"%md ##Schema Inference\n\nWe can also force schema inference\n* ... without creating our own People type, \n* by using a fixed-length data structure (such as a tuple) \n* and supplying the column names to the toDF() method.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"93e77a10-612a-433e-8f51-79159cce25f6"},{"version":"CommandV1","origId":3420809998616669,"guid":"a3610532-8bd3-4a7f-8757-5e55f9a2c236","subtype":"command","commandType":"auto","position":46.0,"command":"%md ##Why do you have to use a tuple?\n* In Python, you don't. You can use any iterable data structure (e.g., a list).\n* In Scala, you do. Tuples have fixed lengths and fixed types for each element at compile time. For instance:\n\n  __```Tuple4[String,String,String,Int]```__\n\n* The DataFrames API uses this information to infer the number of columns and their types. It cannot do that with an array.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"db373823-5e6d-49f3-bfd2-540e121e37b5"},{"version":"CommandV1","origId":3420809998616670,"guid":"d68d4c28-394f-4095-91e2-c285a17c42d3","subtype":"command","commandType":"auto","position":47.0,"command":"val rdd = sc.textFile(\"people.csv\")\n\nval peopleRDD = rdd.map { line =>\n  val cols = line.split(\",\")\n  (cols(0), cols(1), cols(2), cols(3).toInt)\n}\n\nval df = peopleRDD.toDF(\"firstName\", \"lastName\", \"gender\", \"age\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5a8ef10a-0913-42da-9cda-6075a79bfa41"},{"version":"CommandV1","origId":3420809998616671,"guid":"d9e66f36-bf95-4bff-9e33-85a2d726fc14","subtype":"command","commandType":"auto","position":48.0,"command":"%md ##One last method for schemas\n\nInternally, a Dataframe schema is represented by the public StructType type: http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.StructType\n\nYou can use this type to create a schema as in the following example.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b2b5c504-a852-458a-955c-0f64f4095226"},{"version":"CommandV1","origId":3420809998616672,"guid":"3666437f-a505-44dc-b1f0-01f53849be0c","subtype":"command","commandType":"auto","position":49.0,"command":"import org.apache.spark.sql.types._\n\nval schema = StructType(Seq( StructField(\"name\", StringType, false), StructField(\"age\", IntegerType, false) ))","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d3a6df77-039d-49cd-844d-0759a696b025"},{"version":"CommandV1","origId":3420809998616673,"guid":"83180387-91b7-4075-b39c-2e1feacead3f","subtype":"command","commandType":"auto","position":50.0,"command":"%md __But__ ... when and where would that be useful?\n\nMainly when loading files with lots of fields ... case classes and tuples are limited to 22 elements, and lots of real-world tables are hundreds of columns wide.\n\nIn addition, it's easy to build StructType schemas programmatically from a source of schema information -- for example, lots of legacy systems contain files that describe column names and types, so we can create a Spark Dataframe schema by reading that file and building a StructType to apply to corresponding raw data files.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e876c5b2-1755-4067-a126-6648b16e7e04"},{"version":"CommandV1","origId":3420809998616674,"guid":"028d5b67-f2c2-4157-b1da-d7a9f35cc433","subtype":"command","commandType":"auto","position":51.0,"command":"%md ##show()\n\nYou can look at the first n elements in a DataFrame with the show() method. If not specified, n defaults to 20.\n\nThis method is an action. It:\n* reads (or re-reads) the input source\n* executes the RDD DAG across the cluster\n* pulls the n elements back to the driver JVM\n* displays those elements in a tabular form","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"22e2becd-37e3-446a-a9c7-a6912c3040fd"},{"version":"CommandV1","origId":3420809998616675,"guid":"c03fd144-c7a1-404a-b161-cd827b93d3e8","subtype":"command","commandType":"auto","position":52.0,"command":"people.show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d6a1cadd-7837-46cf-8e5e-186e205b06e6"},{"version":"CommandV1","origId":3420809998616676,"guid":"a0e3de30-b735-4fc1-8d45-20f9a941c34d","subtype":"command","commandType":"auto","position":53.0,"command":"%md ##select()\n\nselect() is like a SQL SELECT, allowing you to limit the results to specific columns.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4b982b5e-da8b-4ebf-9398-a26d4b3753ac"},{"version":"CommandV1","origId":3420809998616677,"guid":"5f1ec1ac-b3ee-4a47-a0c5-0740b5558d19","subtype":"command","commandType":"auto","position":54.0,"command":"people.select($\"firstName\", $\"year\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"67043900-38ff-42e3-adba-15772323d937"},{"version":"CommandV1","origId":3420809998616678,"guid":"458dddf8-c24c-4f93-9458-e9fe54478b60","subtype":"command","commandType":"auto","position":55.0,"command":"%md\nselect() also lets you create on-the-fly *derived* columns (similar to SQL select)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"380c812a-6689-4443-9c12-f7dd1043b6f2"},{"version":"CommandV1","origId":3420809998616679,"guid":"cd138a57-9b98-4e35-8c59-2c4fded91a7d","subtype":"command","commandType":"auto","position":56.0,"command":"people.select($\"firstName\", $\"year\", $\"year\" > 1950, $\"year\" + 1000).show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"aad429f5-7758-4ef1-b712-74da9c807b14"},{"version":"CommandV1","origId":3420809998616680,"guid":"b9017f6b-b663-4dde-bb16-242936ba7387","subtype":"command","commandType":"auto","position":57.0,"command":"%md Note that the expressions like ```$\"year\" + 1000``` are also Columns (and that's what you'll see in the DataFrame.select method signature).\n\nThe operators here (> or +) are just methods of Column, and they also return a Column:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ff399c20-de74-4fa2-8a68-b3472ff26760"},{"version":"CommandV1","origId":3420809998616681,"guid":"f03bcea4-ffd4-4639-b45d-60886b5da3bf","subtype":"command","commandType":"auto","position":58.0,"command":"people(\"year\") + 10","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9419998e-67bd-4080-a053-1cef0555daa0"},{"version":"CommandV1","origId":3420809998616682,"guid":"661f6d22-fe87-4816-bcb6-45619337cf7a","subtype":"command","commandType":"auto","position":59.0,"command":"(people(\"year\") + 10) == (people(\"year\").+(10))","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5b63d1cb-6c70-45c0-8648-36d528f6be24"},{"version":"CommandV1","origId":3420809998616683,"guid":"f0129237-de36-4fc4-8134-8bc8b2f34e8d","subtype":"command","commandType":"auto","position":60.0,"command":"people.apply(\"year\").+(10)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5da9eade-a4f6-4ef9-8eef-2cd8cffc2961"},{"version":"CommandV1","origId":3420809998616684,"guid":"7bb4fc15-0bdd-44fd-88f2-aa6762079eee","subtype":"command","commandType":"auto","position":61.0,"command":"%md ... and of course you can use SQL:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8e7056a8-22fd-40c0-9cf9-7acf08beb279"},{"version":"CommandV1","origId":3420809998616685,"guid":"c6b8b5be-a7b8-42c4-8d88-862e0693feb9","subtype":"command","commandType":"auto","position":62.0,"command":"people.registerTempTable(\"people\")\nsqlContext.sql(\"SELECT firstName, year, year + 1000 as future FROM people\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6153ee73-9056-4ff3-875d-38e50ff25cb6"},{"version":"CommandV1","origId":3420809998616686,"guid":"e6be5de6-25ce-4071-a008-7f09b926f3ab","subtype":"command","commandType":"auto","position":63.0,"command":"%md ##filter()\n\nThe filter() method allows you to filter rows out of your results.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9e95f2fd-3693-48bc-94df-d6233103f66a"},{"version":"CommandV1","origId":3420809998616687,"guid":"c96235f5-e060-4d20-bac1-8d7b82f4f464","subtype":"command","commandType":"auto","position":64.0,"command":"people.filter($\"year\" > 1990).select($\"firstName\", $\"year\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"89ac40e5-f53b-468d-91da-44c8f36c4b8f"},{"version":"CommandV1","origId":3420809998616688,"guid":"ccaf9540-055a-4df8-8650-b5fb9c2123fa","subtype":"command","commandType":"auto","position":65.0,"command":"%md Here's how filter appears (as a WHERE clause) in SQL:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"307c6674-98a6-49ed-ae7d-cacf2f2b2234"},{"version":"CommandV1","origId":3420809998616689,"guid":"1fd35719-fd8c-4025-b089-e1c3ba9c9c66","subtype":"command","commandType":"auto","position":66.0,"command":"sqlContext.sql(\"SELECT firstName, year FROM people WHERE year > 1990\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ae2f9778-8e10-4419-b466-bff06b302a00"},{"version":"CommandV1","origId":3420809998616690,"guid":"55342be4-c825-489b-8618-bd2e370569ad","subtype":"command","commandType":"auto","position":67.0,"command":"%md ##orderBy()\n\nThe orderBy() method allows you to sort the results.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2d204cb9-033f-4934-936c-a2445eb66685"},{"version":"CommandV1","origId":3420809998616691,"guid":"f7bdd176-998d-4dfc-871a-dd5d5980e30e","subtype":"command","commandType":"auto","position":68.0,"command":"people.filter(people(\"year\") > 1990).select(people(\"firstName\"), people(\"year\")).orderBy(people(\"year\"), people(\"firstName\")).show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b76a960e-f0f5-4bc0-971d-bcc5ebdaa83a"},{"version":"CommandV1","origId":3420809998616692,"guid":"c7595c85-1342-4fac-9ef0-f716275c128c","subtype":"command","commandType":"auto","position":69.0,"command":"%md It?s easy to reverse the sort order: look for the __desc__ method calls in the following example.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2e3308f6-d3e3-4043-96d6-f5c942594274"},{"version":"CommandV1","origId":3420809998616693,"guid":"408de22f-7945-402c-b408-04cd2e40fb40","subtype":"command","commandType":"auto","position":70.0,"command":"people.filter(people(\"year\") > 1990).select(people(\"firstName\"), people(\"year\")).orderBy(people(\"year\") desc, people(\"firstName\") desc).show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"40bc0714-732a-499d-8037-195a569ae85e"},{"version":"CommandV1","origId":3420809998616694,"guid":"a30b8275-856d-432b-aa70-1f2fe2e0b55d","subtype":"command","commandType":"auto","position":71.0,"command":"%md In SQL, it's pretty normal looking:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"db185670-e80e-49ac-aefc-1a729b279f96"},{"version":"CommandV1","origId":3420809998616695,"guid":"1e82ef9c-b47d-4c52-ac4a-b9bbe09eeccd","subtype":"command","commandType":"auto","position":72.0,"command":"sqlContext.sql(\"SELECT firstName, year FROM people ORDER BY year DESC, firstName DESC\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4a5ac114-dd69-498f-a2c8-80b52f84a1b8"},{"version":"CommandV1","origId":3420809998616696,"guid":"a7281c29-798c-4fa8-bf1c-d2528c10cac4","subtype":"command","commandType":"auto","position":73.0,"command":"%md ##as() or alias()\n\nas() or alias() allows you to rename a column\n\n* it's especially useful with generated columnns (which are sometimes assigned names with non-alphanumeric characters)\n* and joins (to disambiguate columns with same name in each of the joined tables)\n\nnote: in Python you must use \"alias\" because \"as\" is a reserved word","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"73f351fa-e0f2-4bd3-ae94-071123d8fad2"},{"version":"CommandV1","origId":3420809998616697,"guid":"7709631f-5223-47af-81c8-624108ad4877","subtype":"command","commandType":"auto","position":74.0,"command":"people.select($\"firstName\", $\"year\", ($\"year\" > 2000).as(\"recent\")).show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e952ff5c-e10c-4ba7-a55c-32df4fca6cd2"},{"version":"CommandV1","origId":3420809998616698,"guid":"2869ca6b-955d-42d1-920c-49b54e3eac83","subtype":"command","commandType":"auto","position":75.0,"command":"sqlContext.sql(\"SELECT firstName, year, year > 2000 AS recent FROM people\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8de33386-360f-484a-8341-46c6b6002497"},{"version":"CommandV1","origId":3420809998616699,"guid":"489e7ade-bb23-466e-86b8-b13be51e53b7","subtype":"command","commandType":"auto","position":76.0,"command":"%md ##groupBy()\n\ngroupBy() is used to group data (rows) by their value(s) (in one or more specified columns) for aggregations, such as count() or sum()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"59284b62-da24-4167-847d-b5b59e1724a5"},{"version":"CommandV1","origId":3420809998616700,"guid":"9e16915e-d221-4a47-9fbd-ce9a1c04f7f4","subtype":"command","commandType":"auto","position":77.0,"command":"people.groupBy($\"year\").count.show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b34282e4-786e-4764-b705-7d170ad3f3d0"},{"version":"CommandV1","origId":3420809998616701,"guid":"9be20e1a-6a83-4653-b8f6-e39d131bb33e","subtype":"command","commandType":"auto","position":78.0,"command":"import org.apache.spark.sql.functions._\npeople.groupBy($\"year\").agg(sum($\"total\")).show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4956786c-e99d-4e6c-ab45-fa5bda92eb53"},{"version":"CommandV1","origId":3420809998616702,"guid":"75279405-17b5-4661-910e-e4b531c9b872","subtype":"command","commandType":"auto","position":79.0,"command":"sqlContext.sql(\"SELECT year, count(*) AS count, SUM(total) AS total FROM people GROUP BY year\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"61338ba3-f6a7-4e56-9c91-9bdef0e7502b"},{"version":"CommandV1","origId":3420809998616703,"guid":"d1cfe3db-f6be-437d-952d-517ab35b340f","subtype":"command","commandType":"auto","position":80.0,"command":"%md ##Joins\n\nLet's assume we had a file that looked like this:\n\n```\nDacia:Rosella:Samborski:F:1940-08-06:274357:932-39-7400\nLoria:Suzie:Cassino:F:1964-01-31:166618:940-40-2137\nLashaunda:Markita:Rockhill:F:1936-06-02:185766:923-83-5563\nCandace:Marcy:Goike:F:1971-09-25:92497:935-40-2967\nMarhta:Filomena:Bonin:F:1926-06-29:40013:968-22-1158\nRachel:Gwyn:Mcmonigle:F:1951-04-27:211468:926-47-4803\nLorine:Valencia:Bilous:F:2012-09-08:26612:992-10-1262\nAlene:Berniece:Somji:F:1926-04-25:74027:989-16-1381\nSadye:Mara:Morrisseau:F:1930-07-01:209278:971-50-8157\nShawn:Reginia:Battisti:F:1962-08-26:190167:993-42-5846\n```\n\nSuppose we want to join this list against our list of names ... perhaps we'd like to estimate how unusual each first name is.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fdd1ebd0-886e-47d0-ac2b-2d9c257c0408"},{"version":"CommandV1","origId":3420809998616704,"guid":"3267e7e5-315f-425c-bb12-8457420c0433","subtype":"command","commandType":"auto","position":81.0,"command":"val sampleData = sqlContext.read.format(\"com.databricks.spark.csv\")\n  .option(\"delimiter\", \":\")\n  .load(\"dbfs:/mnt/training/dataframes/people.txt\").select($\"_c0\" as \"name\", $\"_c4\" as \"birthdate\")\n\nsampleData.show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f3397be4-496a-4a9f-9623-89913ea27eca"},{"version":"CommandV1","origId":3420809998616705,"guid":"03b7298b-ed81-4e95-b6d6-4faac182cc7c","subtype":"command","commandType":"auto","position":82.0,"command":"%md We can join people to sampleData on the name field with syntax like this:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"637ce330-f2b4-4904-9b87-a86b09e4c740"},{"version":"CommandV1","origId":3420809998616706,"guid":"bbf3771c-4002-4cf1-88aa-cd5875707475","subtype":"command","commandType":"auto","position":83.0,"command":"people.join(sampleData, people(\"firstName\") === sampleData(\"name\")).show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"92b24d21-4545-49cd-b906-eda50c61a0c6"},{"version":"CommandV1","origId":3420809998616707,"guid":"462b81e6-c7cd-4e36-9a75-fff8c340442c","subtype":"command","commandType":"auto","position":84.0,"command":"%md We might be able to add some business value if we can join on the year as well as the name. We can use some built-in DataFrame column functions to extract the year from the (string) birthdate column, and try the join again:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"66107083-72ca-4bec-a549-a52a7123fe78"},{"version":"CommandV1","origId":3420809998616708,"guid":"5214801e-b005-4c3b-b0b8-5e7f91fe2946","subtype":"command","commandType":"auto","position":85.0,"command":"// aside from the DataFrame and Column classes,\n// this is where most of the helper functions live:\nimport org.apache.spark.sql.functions._ \n// split(col, pattern) is located here\n\n// lets us use strongly-typed value IntegerType below:\nimport org.apache.spark.sql.types._ \n\n// cast is a method of the Column class, and withColumn is a method of the DataFrame class:\nval sampleDataWithYear = sampleData.withColumn(\"birthyear\", split($\"birthdate\", \"-\")(0) cast IntegerType)\n\nsampleDataWithYear.show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ae2ef7b6-fc73-451d-8eb0-cbc315f2ac86"},{"version":"CommandV1","origId":3420809998616709,"guid":"f3c437bb-092e-43a1-bf41-a93ac12144f9","subtype":"command","commandType":"auto","position":86.0,"command":"val joined = people.join(sampleDataWithYear, people(\"firstName\") === sampleDataWithYear(\"name\") && people(\"year\") === sampleDataWithYear(\"birthyear\"))\n\njoined.select(\"firstName\", \"total\", \"year\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"eb3eab3e-ca5f-4e84-94c7-bca51872f504"},{"version":"CommandV1","origId":3420809998616710,"guid":"b90e98d5-bb34-4e1b-987b-723c81af7116","subtype":"command","commandType":"auto","position":87.0,"command":"%md ##What Other Handy Functions Can I Use?\n\nIn addition to the members of DataFrame and Column, there are over 100 helpful functions that operate on columns, defined in ```org.apache.spark.sql.functions```\n\nThese include:\n* Date/Time : Convert timestamps, extract fields, arithmetic on dates, parse arbitrary date strings\n* Math : Factorial, log, radians, base change...\n* Conditional : Greatest, least, isnull...\n* String: Base64, regex, length, trim, split, levenshtein...\n\nHere is an example for a common task: converting Unix timestamps into date formatted string, and then into SQL datetime types:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4d74d6f3-7a21-49e1-a4ab-bf2dd8c1e180"},{"version":"CommandV1","origId":3420809998616711,"guid":"072cfece-8f3a-402b-a4c0-050cea04d9f3","subtype":"command","commandType":"auto","position":88.0,"command":"val data = Array((2,1420001316L), (4,1440000006L), (6,1410001316L)) \nval df = sc.parallelize(data).toDF(\"id\", \"timestamp\")\n\nval withStringDate = df.withColumn(\"string_date\", from_unixtime(df(\"timestamp\")))\nval withRealDate = withStringDate.withColumn(\"real_date\", to_utc_timestamp(withStringDate(\"string_date\"), \"GMT\"))\nwithRealDate.show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c5a00cd3-2093-4e41-9d7f-9ceb08ca4869"},{"version":"CommandV1","origId":3420809998616712,"guid":"928e6b7f-7250-4717-80f5-26e1ef74ce57","subtype":"command","commandType":"auto","position":89.0,"command":"%md Note the schema information about the columns in the above output.\n\nBut what if there is just no way to solve our problem with all of these APIs?","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"68a513c1-8d38-42ef-8b14-a06440f9f40f"},{"version":"CommandV1","origId":3420809998616713,"guid":"a3b35d7f-6228-4ea2-a441-61e05d99b468","subtype":"command","commandType":"auto","position":90.0,"command":"%md ##User-Defined Functions\n\nAbove, we extracted the year from the birthdate string by string-splitting and casting using built-in functions.\n\nIf we had an actual date or datetime column, we could also use the built-in year() function to solve the problem.\n\nBut suppose we didn't have either of those options, and we needed to do some custom processing on each row, to produce a new column value.\n\nIn this example, we'd like to apply a function like the following:\n\n```\ndef extractYear(birthdate:String) = birthdate.split(\"-\")(0).toInt\nextractYear(\"2009-10-31\")\n\nres42: Int = 2009\n```\n\nWe can create a function like that, and convert it to a Spark UDF that operates on (and returns a column):","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"90a037b2-c64e-46f5-8424-101c02803b7b"},{"version":"CommandV1","origId":3420809998616714,"guid":"9c7ad806-cffb-471c-90ca-75dec398c594","subtype":"command","commandType":"auto","position":91.0,"command":"val extractYear = sqlContext.udf.register(\"pullOutYear\", (birthdate:String) => birthdate.split(\"-\")(0).toInt)\n\nsampleData.select($\"name\", $\"birthdate\", extractYear($\"birthdate\") as \"birthyear\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ca9deb7e-1743-4ab3-9634-25cdf8f94e66"},{"version":"CommandV1","origId":3420809998616715,"guid":"f320751f-bef9-4fb8-b524-60830cab69f4","subtype":"command","commandType":"auto","position":92.0,"command":"%md What's that ```pullOutYear``` string doing in there? That name is available to SQL commands:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d2bcdd61-7a7a-4c0a-bb68-0af071444b83"},{"version":"CommandV1","origId":3420809998616716,"guid":"2ff97544-f1a3-4592-89bf-ba4a8aba20e7","subtype":"command","commandType":"auto","position":93.0,"command":"sampleData.registerTempTable(\"sampleData\")\nsqlContext.sql(\"SELECT name, birthdate, pullOutYear(birthdate) AS birthyear FROM sampleData\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"e4401865-704c-410f-b4a5-e8db5453fdc6"},{"version":"CommandV1","origId":3420809998616717,"guid":"2777f730-8923-43d9-a246-966deb15a7d0","subtype":"command","commandType":"auto","position":94.0,"command":"%md __It is strongly preferable to use a combination of built-in functions, rather than defining your own.__\n\nWhy? Besides being (usually) easier, Spark *cannot optimize your UDF functions*\n\nIn fact, Spark cannot \"see inside\" your UDF functions at all. This means, for example, that a join on a UDF will require a Cartesian join, then testing every pair of values against your UDF. \n\nIn addition, most built-in functions include support for code generation, which offers significant speed improvements.\n\nBut if a UDF is the only reasonable solution, it is easy to add one.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0807fb5d-e722-40b8-b870-08f87802283b"},{"version":"CommandV1","origId":3420809998616718,"guid":"29b332d5-1710-4c65-8d4f-e73619629dfd","subtype":"command","commandType":"auto","position":95.0,"command":"%md ##SparkSQL: Just a Little More Info\n\nAs we've seen, Spark SQL operations generally return DataFrames. So, in addition to starting with a DataFrame, calling registerTempTable, and then executing SQL, like we did above ... we can also go the other direction, starting with SQL and switching to DataFrames whenever we'd like:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"54c386a9-ce5f-437b-8572-333c44f14404"},{"version":"CommandV1","origId":3420809998616719,"guid":"faeb9950-cee7-4462-89ef-d94717c269ac","subtype":"command","commandType":"auto","position":96.0,"command":"%sql CREATE TEMPORARY TABLE more_people \nUSING parquet \nOPTIONS (path \"dbfs:/mnt/training/ssn/names.parquet\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"884c73a9-9a04-45ca-8cfc-6a1f5cea5d7d"},{"version":"CommandV1","origId":3420809998616720,"guid":"751ca55f-c6ef-47df-b72b-35ca90781be9","subtype":"command","commandType":"auto","position":97.0,"command":"sqlContext.table(\"more_people\").filter($\"year\" > 2010).show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b8bab669-9a48-4ddf-a643-b726f80429e8"},{"version":"CommandV1","origId":3420809998616721,"guid":"59dde4c4-e04a-4925-b8f0-24f006723483","subtype":"command","commandType":"auto","position":98.0,"command":"%md Because these operations return DataFrames, all the usual DataFrame operations are available.\n\n. . . including the ability to create new temporary tables:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c21cd382-ed57-4a8c-8fea-cbadc05f0175"},{"version":"CommandV1","origId":3420809998616722,"guid":"80379f26-f987-4dc4-b666-f30e6dfde7b7","subtype":"command","commandType":"auto","position":99.0,"command":"sqlContext.sql(\"SELECT * FROM more_people WHERE year = 2014\").registerTempTable(\"names2014\")\n\nsqlContext.table(\"names2014\").show(5)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a0828c78-ab8a-4720-99c0-c9167c7dac54"},{"version":"CommandV1","origId":3420809998616723,"guid":"5e9358e4-9c1e-41b3-ae26-567527bc9be9","subtype":"command","commandType":"auto","position":100.0,"command":"%md ##Window Functions\n\nAllow us to query over ranges or windows within data set\n* Supported in many SQL database environments\n* Any aggregate function can be used, in addition to window-specific ranking and analytic functions\n* Spark Window Functions work with SQL or DSL\n* In DSL, create a WindowSpec using Window helper object, then use windowfunction's column .over(windowspec)\n\nHere's a typical task: calculate rankings within multiple groups -- in this example, simultaneously query rankings of sports teams in their respective divisions:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"189fd6c5-fdfd-4709-b171-8fafa023c778"},{"version":"CommandV1","origId":3420809998616724,"guid":"3ad00f8c-664a-432a-9363-4f14d9c35827","subtype":"command","commandType":"auto","position":101.0,"command":"val data = Array((\"Bears\", \"E\", 10), (\"Lizards\", \"W\", 8), (\"Giraffes\", \"W\", 7), (\"Tigers\", \"E\", 9), (\"Crickets\", \"E\", 6), (\"Bats\", \"W\", 6)) \nval df = sc.parallelize(data).toDF(\"team\", \"division\", \"wins\")\ndf.registerTempTable(\"scores\") //for SQL\n\nval rankings = sqlContext.sql(\"SELECT team, division, wins, rank() OVER (PARTITION BY division ORDER BY wins desc) as rank FROM scores\")\nrankings.show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bae31a0c-1de2-4465-8a17-38cd3a1c1000"},{"version":"CommandV1","origId":3420809998616725,"guid":"8c011292-198e-4b87-843a-b6af9883e4d3","subtype":"command","commandType":"auto","position":102.0,"command":"%md In the DataFrame API, we need to create a \"WindowSpec\" object first, then call ```rank().over(windowSpec)```:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"46e16a00-6cb2-4f5b-960c-64d55b79f5d0"},{"version":"CommandV1","origId":3420809998616726,"guid":"021ceb7a-0870-4a07-b011-a6098f1c6d04","subtype":"command","commandType":"auto","position":103.0,"command":"import org.apache.spark.sql.expressions._\nimport org.apache.spark.sql.functions._\n\nval ws = Window.partitionBy(df(\"division\")).orderBy(-df(\"wins\"))\ndf.withColumn(\"rank\", rank().over(ws)).show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"02826100-2e95-437a-9fc5-833d67b7e718"},{"version":"CommandV1","origId":3420809998616727,"guid":"d0ae91f7-ebf3-43c2-8f7d-1e9b1e70a274","subtype":"command","commandType":"auto","position":104.0,"command":"%md ##Supported Window Functions\n\n| |SQL|DataFrame API|\n|-|---|-------------|\n|Ranking functions|rank|rank|\n||dense\\_rank|denseRank|\n||percent\\_rank|percentRank|\n||ntile|ntile|\n||row\\_number|rowNumber|\n|Analytic functions|cume\\_dist|cumeDist|\n||first\\_value|firstValue|\n||last\\_value|lastValue|\n||lag|lag|\n||lead|lead|\n\nhttps://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"74884540-a90d-44ac-9415-cccb87406872"},{"version":"CommandV1","origId":3420809998616728,"guid":"25256a6a-3919-44cc-8919-761489cbf268","subtype":"command","commandType":"auto","position":105.0,"command":"%md ##User-Defined Aggregation Functions (UDAF)\n\nUser-defind functions as discussed earlier apply to one or more columns, but only in one row (record) at a time. So they are, effectively, map functions.\n\n####What about reduce-style custom functions, that can operate on more than one records at a time?\n\nIf we want to reduce, or aggregate, data from multiple rows in a DataFrame, and we cannot solve the problem with the built-in aggregations, Spark allows us to create User-Defined Aggregation Functions, or UDAFs\n\nSince this sort of functions needs to encapsulate a parallelizable reduce, it's a bit more complex. We need to implement a class that extends ```org.apache.spark.sql.UserDefinedAggregateFunction``` and implements 8 abstract methods.\n\nOnce we have a class that meets the requirements and implements our logic, we can instantiate it and use the instance just as we would one of the built-in aggregations like \"sum\" or \"count\" -- e.g., we can groupBy one or more columns, then aggregate over another.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"06cec26a-7d65-4aa0-b0c7-37e98cf60698"},{"version":"CommandV1","origId":3420809998616729,"guid":"7c4f2deb-bf2b-4383-aa5d-7c8299e2cb33","subtype":"command","commandType":"auto","position":106.0,"command":"%md In the example below, we will create a UDAF that collects all of the values of a particular column in all of the rows in a group. That is, we can groupBy one column and then \"collect\" all of the different values that appear (within the group) in a second column, and place those in an Array. This example will treat those values as Strings, its value will be Array[String]\n\nDescribed as code, we'd like to start with data like\n\n```val a = sc.parallelize(List((\"foo\", \"abc\"), (\"bar\", \"abc\"), (\"foo\", \"def\"))).toDF(\"name\", \"bonus\")```\n\nand then, if we group by the name column, we'll get a row for name \"foo\" where the aggregated column has a string array of \"abc\" and \"def\"\n\nSimilarly, we could group by the \"bonus\" column, and then we'll get a row for the bonus value \"abc\" where the aggregated column has a string array of the name values \"foo\" and \"bar\":","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c5b31fcc-80e8-486d-bedc-c4233f939f64"},{"version":"CommandV1","origId":3420809998616730,"guid":"e0bfea94-339d-4438-a07f-88b97e0c5249","subtype":"command","commandType":"auto","position":107.0,"command":"import org.apache.spark.sql.expressions.MutableAggregationBuffer\nimport org.apache.spark.sql.expressions.UserDefinedAggregateFunction\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n \nclass Collect extends UserDefinedAggregateFunction {\n  def inputSchema: org.apache.spark.sql.types.StructType =\n    StructType(StructField(\"name\", StringType) :: Nil)\n \n  def bufferSchema: StructType = StructType(\n    StructField(\"strings\", ArrayType(StringType, false)) :: Nil\n  )\n \n  def dataType: DataType = ArrayType(StringType)\n  def deterministic: Boolean = true\n  def initialize(buffer: MutableAggregationBuffer): Unit = {\n    buffer(0) = Array[String]()\n  }\n \n  def update(buffer: MutableAggregationBuffer,input: Row): Unit = {\n    buffer(0) = buffer.getSeq(0) :+ input.getAs[String](0)\n  }\n \n  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n   buffer1(0) = buffer1.getSeq(0) ++ buffer2.getSeq(0)\n  }\n \n  def evaluate(buffer: Row): Any = {\n    buffer.getAs[Array[String]](0)\n  }\n}\n\nval collect = new Collect","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fb277107-0bb6-4832-9606-eec5d34cee20"},{"version":"CommandV1","origId":3420809998616731,"guid":"79fd400d-383f-40f7-a31b-3872d9aae4d3","subtype":"command","commandType":"auto","position":108.0,"command":"%md Now we can use the instance \"collect\" as a UDAF and aggregate with it:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f7795987-cf45-4996-93ca-6749dc729f60"},{"version":"CommandV1","origId":3420809998616732,"guid":"a064dd8f-9c04-4cc1-bd7d-07a96bf0d678","subtype":"command","commandType":"auto","position":109.0,"command":"val a = sc.parallelize(List((\"foo\", \"abc\"), (\"bar\", \"abc\"), (\"foo\", \"def\"))).toDF(\"name\", \"bonus\")\na.groupBy($\"bonus\").agg(collect($\"name\")).show\na.groupBy($\"name\").agg(collect($\"bonus\")).show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f3841c16-2bb8-4f0d-ba05-cc556b501049"},{"version":"CommandV1","origId":3420809998616733,"guid":"77d6f673-06b5-4662-9355-9c727241add1","subtype":"command","commandType":"auto","position":110.0,"command":"a.groupBy($\"name\").agg(collect($\"bonus\")).printSchema","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a2920456-4a50-4cb4-93c8-97e8cca94e47"},{"version":"CommandV1","origId":3420809998616734,"guid":"5a392fbc-6086-4c9a-b70c-59badeb42c9c","subtype":"command","commandType":"auto","position":111.0,"command":"%md ##Complex Column Types\n\nDataframes can support complex types in a column. For example, a column's type can be a hashmap. It is common to encounter map-typped columns in an existing Hive environment, although Spark's support *does not require Hive*.\n\nThere are several ways to access data in a map-typed column, depending upon whether we want to retrieve the map itself, or instead transform the Dataframe to bring map keys, values, or both up to top-level fields in the schema.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a0c4bc8d-08cc-4a6c-9086-57cf403224b2"},{"version":"CommandV1","origId":3420809998616735,"guid":"69fb0b8d-206e-491d-b151-1057bd304d04","subtype":"command","commandType":"auto","position":112.0,"command":"val df = sc.parallelize(List( (\"John\", Map(\"games\"->12, \"highscore\" ->199) ), (\"Anne\", Map(\"games\"->9, \"highscore\" ->200 )  ))).toDF(\"name\", \"player_info\")\ndf.printSchema\ndf.show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f06c88f3-9a67-4d65-b32a-f3c44102d167"},{"version":"CommandV1","origId":3420809998616736,"guid":"648cc7d9-759c-42bb-835d-982b2bc52576","subtype":"command","commandType":"auto","position":113.0,"command":"%md The following code shows how the map is accessed within a Row. Although \"collect\" will rarely be appropriate in big data settings, the focus here is on Row -- a Dataframe is a Dataset[Row] and can be transparently converted to a RDD[Row] if necessary.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"925afa47-a27d-4165-aaa5-dca274e63508"},{"version":"CommandV1","origId":3420809998616737,"guid":"2e2a4232-310c-49d8-8254-788047bc2f8b","subtype":"command","commandType":"auto","position":114.0,"command":"val rows = df.filter($\"name\" === \"John\").select($\"player_info\").collect\nval map = rows(0).getMap[String,Int](0)\nmap(\"games\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"af10343f-eb62-4453-b0a3-ea945b81a8c1"},{"version":"CommandV1","origId":3420809998616738,"guid":"d66c9677-62f8-4d4b-bd02-400952ea4bb9","subtype":"command","commandType":"auto","position":115.0,"command":"%md If we know the specific key we need from a map, we can project it into a top-level column using a \".\" syntax within a column name:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d3cb4bc6-1881-4d07-93d6-8cf7e354104b"},{"version":"CommandV1","origId":3420809998616739,"guid":"185a8d39-ed3c-4bcd-893e-c0f751f3ee6c","subtype":"command","commandType":"auto","position":116.0,"command":"df.select($\"name\", $\"player_info.highscore\").show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c21d813b-6851-47e0-9ee7-bec04c714f65"},{"version":"CommandV1","origId":3420809998616740,"guid":"8c7688ac-e255-4b4e-add4-d2c7e77ccd21","subtype":"command","commandType":"auto","position":117.0,"command":"%md We can use the `explode` method to create a custom translation of a Map (or other structure) into one or more columns and/or rows. This is similar to a HiveQL `LATERAL VIEW`\n\nFirst, let's look at making one fixed column with custom String contents for the map: we can make 1 or more Rows from the map.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9f38b2ec-0741-40a6-acdf-acd13525d3bb"},{"version":"CommandV1","origId":3420809998616741,"guid":"dda8c3ee-2238-4604-8d2d-ce5f8d69a0f1","subtype":"command","commandType":"auto","position":118.0,"command":"df.select($\"name\", $\"player_info\").explode(\"player_info\", \"entry\") {\n  player_info_map : Map[String,Int] => {\n    player_info_map.keys.map(k => k + \" : \" + player_info_map(k))\n  }\n}.show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5a4b35c4-6352-42bd-a7ab-906f6316d2f7"},{"version":"CommandV1","origId":3420809998616742,"guid":"892e7ee4-4320-42f5-9343-8e8198c3e14b","subtype":"command","commandType":"auto","position":119.0,"command":"%md Sometimes, we'd like to make a number of new columns, perhaps representing specific keys that were in the map. We can do that too:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3e920351-6428-43aa-9ab9-11afae887b71"},{"version":"CommandV1","origId":3420809998616743,"guid":"ff202c66-5b4c-4c99-a313-ba4a61d1e2f6","subtype":"command","commandType":"auto","position":120.0,"command":"case class PlayerInfo(games:Int, highscore:Int)\n\ndf.select($\"name\", $\"player_info\").explode($\"player_info\"){\n  r: Row => {\n    val map = r.getMap[String,Int](0)    \n    Seq(PlayerInfo(map(\"games\"), map(\"highscore\")))\n  }\n}.show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7aaa707b-2421-4159-aad3-77b39494e06b"},{"version":"CommandV1","origId":3420809998616744,"guid":"ec280af8-1848-4d8e-800e-343ad304bb00","subtype":"command","commandType":"auto","position":121.0,"command":"%md What if we want to expand each key-value pair in into its own row, a bit like a triple-store (https://en.wikipedia.org/wiki/Triplestore) ?\n\nWe can do that with the same `explode` call, by returning an iterator with different semantics:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"5ed8bb5f-7344-4f44-b5c8-81aba7335c80"},{"version":"CommandV1","origId":3420809998616745,"guid":"09ad7243-d9d3-42e5-ba34-91737e871933","subtype":"command","commandType":"auto","position":122.0,"command":"df.select($\"name\", $\"player_info\").explode($\"player_info\"){\n  r: Row => {\n    val map = r.getMap[String,Int](0)\n    map.keys.map(k => (k, map(k)))\n  }\n}.withColumnRenamed(\"_1\", \"map_key\").withColumnRenamed(\"_2\", \"map_value\").show","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0e69c32b-0228-496f-9280-a6a6cf2c3146"},{"version":"CommandV1","origId":3420809998616746,"guid":"6268ecdd-91ea-40f2-993e-033a85cc57ce","subtype":"command","commandType":"auto","position":123.0,"command":"%md ##DataFrame Limitations\n\n* Spark does not automatically repartition DataFrames optimally during shuffles\n  * During a DF shuffle, SparkSQL will just use ```spark.sql.shuffle.partitions``` to determined the number of partitions in the downstream RDD\n  * All SQL configurations can be changed, including this one\n    * via ```sqlContext.setConf(key, value)```\n    * or in Databricks: ```%sql SET key=val```","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d0f6e5ab-be5e-4520-98db-8a4251f602f3"},{"version":"CommandV1","origId":3420809998616747,"guid":"da0af0a2-ab20-4970-9713-974440fb9094","subtype":"command","commandType":"auto","position":124.0,"command":"%md ##Machine Learning Integration\n\n* Spark 1.2 introduced a new package called __spark.ml__, which aims to provide a uniform set of high-level APIs that help users create and tune practical machine learning pipelines.\n\n* Spark ML standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single *pipeline*, or *workflow*.\n\n* Spark ML uses DataFrames as a dataset which can hold a variety of data types. \n  * For instance, a dataset could have different columns storing text, feature vectors, true labels, and predictions.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"603f20a1-93b2-414a-aa94-65896605f87f"}],"dashboards":[],"guid":"a958579a-3bcf-4c08-95c0-a447b3d64aac","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":0,"guid":"40a48cbd-4823-4a33-a10f-af47d303e302","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{}};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>